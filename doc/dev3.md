# StreamMind 项目整体方案

## 1. 需求描述

### 核心需求

通过浏览器摄像头实时录制用户活动（编程、手工、教学等任何场景），AI自动生成连贯的文字记录。

### 关键特征

- **通用性**：不限于特定场景
- **连贯性**：AI理解上下文，生成连贯描述而非碎片
- **准确性**：优先保证质量，可接受半实时延迟（30秒-2分钟）
- **完整性**：不遗漏关键信息

### 非需求

- 不追求毫秒级实时反馈
- 不保存原始视频（MVP阶段）
- 不做复杂的视频预处理

---

## 2. 整体架构

```
┌─────────────┐
│  前端Web    │  录制+上传
└──────┬──────┘
       │ 30秒视频片段
       ↓
┌─────────────┐
│Spring Boot  │  业务中枢
│  - API网关  │  - 调度协调
│  - 队列管理 │  - 数据持久化
│  - 结果汇聚 │
└─┬─────────┬─┘
  │         │
  ↓         ↓
┌──────┐  ┌──────────┐
│Python│  │PostgreSQL│
│AI处理│  │          │
└──────┘  └──────────┘
```

### 分工原则

- **前端**：采集和上传，不处理视频
- **Spring Boot**：业务逻辑、任务调度、数据管理
- **Python**：AI分析（视频→文字）
- **不做的事**：提取关键帧、视频编辑、复杂预处理

---

## 3. 技术栈

### 前端

- **MediaRecorder API**：浏览器原生视频录制
- **WebM格式**：浏览器原生支持
- **HTTP上传**：简单可靠

### Spring Boot（业务中枢）

- **Spring Boot 3.x**
- **Spring WebFlux**（响应式）
- **Spring Data JPA**
- **内置任务队列**或 **RabbitMQ**（可选）
- **WebSocket**（推送结果）

### Python AI服务

- **FastAPI**
- **Qwen2.5-VL**或**Gemini 2.0**
- **FFmpeg**（仅用于视频切片）
- **gRPC**（与Spring Boot通信）

### 数据存储

- **PostgreSQL**：会话和分析记录
- **文件系统**：临时视频存储

---

## 4. 核心流程

### 录制与上传

```
浏览器
 → getUserMedia（获取摄像头）
 → MediaRecorder（录制30秒）
 → HTTP POST（上传WebM视频）
 → 继续下一个30秒
```

### Spring Boot 协调

```
接收视频
 → 保存临时文件
 → 创建任务（sessionId + videoPath）
 → 放入队列
 → 返回"已接收"

Worker线程：
 → 从队列取任务
 → 调用Python服务分析
 → 接收流式结果
 → 保存数据库
 → WebSocket推送前端
```

### Python AI处理

```
接收分析请求（视频路径 + sessionId）
 → FFmpeg切片（滑动窗口）
    例：30秒视频 → [0-15s] [10-25s] [20-30s]
 → 逐个窗口分析：
    Qwen2.5-VL.analyze(video_file, context)
 → 流式返回token
 → gRPC发送给Spring Boot
```

---

## 5. 滑动窗口策略

### 目的

保证动作完整性和上下文连贯性

### 实现

```
输入：30秒视频
窗口大小：15秒
步长：10秒

切片结果：
- window_1: 0-15秒
- window_2: 10-25秒  ← 与window_1重叠5秒
- window_3: 20-30秒

每个窗口 = 完整的视频文件
直接给AI分析，不提取帧
```

### 参数权衡

- **窗口太小**（<10秒）：动作可能不完整
- **窗口太大**（>30秒）：AI处理慢，token消耗大
- **重叠太多**：冗余计算
- **重叠太少**：可能截断关键动作

**推荐**：15秒窗口，10秒步长（33%重叠）

---

## 6. 上下文管理

### 问题

每个窗口独立分析，如何保证连贯？

### 方案

```
第1个窗口：
prompt = "这是0-15秒的视频，描述发生了什么"

第2个窗口：
prompt = """
这是10-25秒的视频
之前0-15秒的内容是：{前一个结果}
描述10-25秒新增的内容
"""

第N个窗口：
prompt = """
这是T1-T2秒的视频
之前的摘要：{历史摘要}
描述当前窗口的内容
"""
```

### 策略

- 短期记忆：携带上一窗口的完整结果
- 长期记忆：定期总结压缩，避免token超限
- AI负责理解连贯性，不需要人工做复杂的上下文拼接

---

## 7. 通信协议

### 前端 → Spring Boot

- **HTTP POST**：上传视频
- **WebSocket**：接收实时结果

### Spring Boot ↔ Python

- **gRPC**（推荐）：
    - 双向流式
    - 类型安全
    - 高性能
- 或 **HTTP + Server-Sent Events**

### Python → Spring Boot

```
流式返回：
for token in AI.analyze(video):
    grpc.send(sessionId, token, timestamp)
```

---

## 8. 数据模型

### 核心表

**sessions**

```
id, user_id, status, start_time, end_time
```

**analysis_records**

```
id, session_id, content, window_index, timestamp
```

### 查询逻辑

```sql
SELECT * FROM analysis_records 
WHERE session_id = ? 
ORDER BY timestamp ASC
```

拼接生成完整记录

---

## 9. 性能考虑

### 延迟预估

```
录制30秒 → 上传(1秒) → 队列(<1秒) 
→ FFmpeg切片(1秒) → AI分析(10-30秒/窗口)

首次反馈：约45-90秒
后续反馈：每10-30秒一条
```

### 瓶颈

- **AI分析速度**：主要瓶颈
- **队列积压**：如果AI太慢

### 优化方向

- 增加Python Worker数量（并行）
- 使用GPU加速AI推理
- 调整窗口大小和步长
- 使用更快的AI模型（可能牺牲质量）

---

## 10. 核心原则

### 做什么

✅ 录制完整视频（30fps）  
✅ 滑动窗口切片  
✅ AI直接分析视频  
✅ 流式返回结果  
✅ 保持上下文连贯

### 不做什么

❌ 提取关键帧  
❌ 手动视频编辑  
❌ 复杂的预处理  
❌ 实时截图  
❌ 图片序列输入

**原则**：把视频直接交给AI，让AI决定如何采样和理解


---

## 12. 风险与限制

### 技术限制

- AI分析速度无法突破（硬件限制）
- 视频文件大小和网络带宽
- 浏览器MediaRecorder兼容性

### 质量权衡

- 窗口越大越准确，但越慢
- 重叠越多越完整，但计算冗余
- 需要根据实际场景调优

---

## 总结

这是一个以Spring
Boot为中枢的半实时视频分析系统。核心思路是让浏览器录制完整视频，定期上传，后端用滑动窗口切片后交给AI直接分析，避免手动处理视频的复杂性。优先保证分析质量和完整性，可接受延迟。